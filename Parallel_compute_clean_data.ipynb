{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import category_encoders as ce\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import gensim.downloader as api\r\n",
    "\r\n",
    "def hash_encode_username(df_t):\r\n",
    "    \"\"\"[hash binning of username into 512 bins]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.DataFrame]): [Input dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.DataFrame]: [returns dataframe with username replaced]\r\n",
    "    \"\"\"\r\n",
    "    encoder=ce.HashingEncoder(cols='Username',n_components=512,max_process=1)\r\n",
    "    newdf=encoder.fit_transform(df_t)\r\n",
    "    return newdf\r\n",
    "def clean_timestamp(df_t):\r\n",
    "    \"\"\"[converts timestamp data from string to pandas datetime]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.DataFrame]): [Input Dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.DataFrame]: [returns dataframe with timestamp column cleaned]\r\n",
    "    \"\"\"\r\n",
    "    df_t['Timestamp']=pd.to_datetime(df_t['Timestamp'])\r\n",
    "    return df_t\r\n",
    "def create_entitylist(df_t):\r\n",
    "    \"\"\"[Converts entity list into column with with mean of entity confidence score\r\n",
    "    , median of entity confidence score and max of entity confidence score. Also\r\n",
    "    converts the entity into glove embedding, for one of them whenever possible\r\n",
    "     ]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.DataFrame]): [Input DataFrame]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.DataFrame]: [return Dataframe with entity column processed]\r\n",
    "    \"\"\"\r\n",
    "    comblist=[]\r\n",
    "    comblist2=[]\r\n",
    "    for i in df_t.Entities.str.split(';').tolist():\r\n",
    "        newlist=[]\r\n",
    "        stringlist=[]\r\n",
    "        for j in i:\r\n",
    "            if j=='null' or j=='':\r\n",
    "                'here'\r\n",
    "            else:\r\n",
    "                newlist.append(j.split(\":\")[-1])\r\n",
    "                stringlist.append(j.split(\":\")[1])\r\n",
    "        newlist = newlist[:42]+ [None]*(42 - len(newlist[:42]))\r\n",
    "        comblist.append(newlist[:-1])\r\n",
    "        comblist2.append(stringlist)\r\n",
    "    newdf=pd.DataFrame(comblist)\r\n",
    "    newdf = newdf.apply(pd.to_numeric)\r\n",
    "    newdf2=newdf.mean(axis=1)\r\n",
    "    newdf3=newdf.median(axis=1)\r\n",
    "    newdf4=newdf.max(axis=1)\r\n",
    "    df_t=pd.concat([df_t,newdf2], axis=1)\r\n",
    "    df_t=pd.concat([df_t,newdf3], axis=1)\r\n",
    "    df_t=pd.concat([df_t,newdf4], axis=1)\r\n",
    "\r\n",
    "\r\n",
    "    dfnew=newdf.idxmax(axis=1)\r\n",
    "    df_t=df_t.drop('Entities',axis=1)\r\n",
    "    slist=[]\r\n",
    "    corpus=ray.get(stored_model)\r\n",
    "    for index,strings in enumerate(comblist2):\r\n",
    "        if strings!=[]:\r\n",
    "            if corpus.has_index_for(strings[int(dfnew.iloc[index])].lower()):\r\n",
    "                embed=corpus[strings[int(dfnew.iloc[index])].lower()]\r\n",
    "            else:\r\n",
    "                embed=None\r\n",
    "            count=0\r\n",
    "            while embed is None and count!=len(strings):\r\n",
    "                if corpus.has_index_for(strings[count].lower()):\r\n",
    "                    embed=corpus[strings[count].lower()]\r\n",
    "                else:\r\n",
    "                    embed=None            \r\n",
    "                count+=1\r\n",
    "        else:\r\n",
    "            embed=None\r\n",
    "            \r\n",
    "    if embed is  None:\r\n",
    "        embed=[None]*200\r\n",
    "    slist.append(embed)\r\n",
    "    sdf=pd.DataFrame(slist)\r\n",
    "    df_t=pd.concat([df_t, sdf], axis=1)\r\n",
    "\r\n",
    "    return df_t\r\n",
    "            \r\n",
    "def split_sentiment(df_t):\r\n",
    "    \"\"\"[Split sentiment columns into 2 with only their absolutes remaining]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.DataFrame]): [Input Dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.DataFrame]: [return Dataframe with sentiment column processed]\r\n",
    "    \"\"\"\r\n",
    "    newdf=pd.DataFrame(df_t.Sentiment.str.split('-').tolist())\r\n",
    "    newdf = newdf.apply(pd.to_numeric)\r\n",
    "    df_t=df_t.drop('Sentiment',axis=1)\r\n",
    "    df_t=pd.concat([df_t, newdf], axis=1)\r\n",
    "    return df_t\r\n",
    "            \r\n",
    "def hash_encode_column(df):\r\n",
    "    \"\"\"[hash encode column into 64 columns]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df ([pd.Series]): [Input a series and return a dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.Dataframe]: [Dataframe of series converted into 64 columns]\r\n",
    "    \"\"\"\r\n",
    "    encoder=ce.HashingEncoder(n_components=64,max_process=1)\r\n",
    "    newdf=encoder.fit_transform(pd.DataFrame(df))\r\n",
    "    return newdf\r\n",
    "def hash_encode_column2(df):\r\n",
    "    \"\"\"[hash encode column(pd.series) into 256 columns]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df ([pd.Series]): [Input a series and return a dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.Dataframe]: [Dataframe of series converted into 256 columns]\r\n",
    "    \"\"\"\r\n",
    "    encoder=ce.HashingEncoder(n_components=256,max_process=1)\r\n",
    "    newdf=encoder.fit_transform(pd.DataFrame(df))\r\n",
    "    return newdf\r\n",
    "def clean_general(df_t,column):\r\n",
    "    \"\"\"[hash encode a column after splitting the column into a list.]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.Dataframe]): [input dataframe]\r\n",
    "        column ([string]): ['column name to be cleaned']\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.DataFrame]: [Return the dataframe after it has been hash encoded and the column has been \r\n",
    "        converted into a list]\r\n",
    "    \"\"\"\r\n",
    "    totallist=[]\r\n",
    "    listtoprocess=df_t[column].str.split(' ').tolist()\r\n",
    "    for j in listtoprocess:\r\n",
    "        if j==None:\r\n",
    "            totallist.append([None])\r\n",
    "        else:\r\n",
    "            if j[0]==None or j[0]==\"null;\" or j[0]==\"\":\r\n",
    "                totallist.append([None])\r\n",
    "            else:\r\n",
    "                totallist.append([j[0]])\r\n",
    "    df=pd.DataFrame(totallist)\r\n",
    "    df_t=pd.concat([df_t,hash_encode_column(df)], axis=1)\r\n",
    "    return df_t\r\n",
    "def count_general(df_t,column):\r\n",
    "    \"\"\"[count how many items are there in the list for 1 column]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.Dataframe]): [input dataframe]\r\n",
    "        column ([string]): ['column name to be cleaned']\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.DataFrame]: [Return the dataframe with the input column counted and replaced]\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    df_t[column]=df_t[column].str.split(' ').str.len() \r\n",
    "    return df_t[(df_t[column]>270)==False]\r\n",
    "    \r\n",
    "def clean_url(df_t):\r\n",
    "    \"\"\"[Clean the Url column and hash encode the first item in the list for that column]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.Dataframe]): [Input Dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.Dataframe]: [output Dataframe with url column cleaned, converted to list and encoded]\r\n",
    "    \"\"\"\r\n",
    "    p = re.compile(\":\\/\\/(.*?)\\/|:\\/\\/(.*?)$\")\r\n",
    "    totallist=[]\r\n",
    "    listtoprocess=df_t['URLs'].str.split(':-:').tolist()\r\n",
    "    for j in listtoprocess:\r\n",
    "        if j==None:\r\n",
    "            totallist.append([None])\r\n",
    "        else:\r\n",
    "            if j[0]==None or j[0]==\"null;\" or j[0]==\"\":\r\n",
    "                totallist.append([None])\r\n",
    "            else:\r\n",
    "                totallist.append([p.findall(j[0])[0][0]])\r\n",
    "    df=pd.DataFrame(totallist)\r\n",
    "    df_t=pd.concat([df_t,hash_encode_column2(df)], axis=1)\r\n",
    " \r\n",
    "\r\n",
    "    return df_t\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "def count_url(df_t):\r\n",
    "    \"\"\"[count the number of url after the column has been processed to lists]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df_t ([pd.Dataframe]): [Input Dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "       [pd.DataFrame]: [Return the dataframe with the url column counted and replaced]\r\n",
    "    \"\"\"\r\n",
    "    df_t['URLs']=df_t.URLs.str.split(' ').str.len() \r\n",
    "    return df_t\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def split_timestamp_to_cols(df):\r\n",
    "    \"\"\"[Convert Timestamp into sin and consine second and months to \r\n",
    "    find seasonality patterns within the data for the models to pick up]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df ([pd.DataFrame]): [Input Dataframe]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [pd.Dataframe]: [returns dataframe with timestamp converted]\r\n",
    "    \"\"\"\r\n",
    "  total_seconds_in_day = 60 * 60 * 24\r\n",
    "  total_months_in_year = 12\r\n",
    "\r\n",
    "  timestamps = df.Timestamp\r\n",
    "  second_of_day = pd.to_datetime(timestamps).dt.second + \\\r\n",
    "                  pd.to_datetime(timestamps).dt.minute * 60 + \\\r\n",
    "                  pd.to_datetime(timestamps).dt.hour * 60 * 60\r\n",
    "  \r\n",
    "  df[\"sin_second\"] = np.sin(2*np.pi*second_of_day/total_seconds_in_day)\r\n",
    "  df[\"cos_second\"] = np.cos(2*np.pi*second_of_day/total_seconds_in_day)\r\n",
    "\r\n",
    "  month_of_year = pd.to_datetime(timestamps).dt.month\r\n",
    "\r\n",
    "  df[\"sin_month\"] = np.sin(2*np.pi*month_of_year/total_months_in_year)\r\n",
    "  df[\"cos_month\"] = np.cos(2*np.pi*month_of_year/total_months_in_year)\r\n",
    "  \r\n",
    "  df[\"year\"] = pd.to_datetime(timestamps).dt.year\r\n",
    "\r\n",
    "  df.drop(labels=\"Timestamp\", axis=\"columns\", inplace=True)\r\n",
    "\r\n",
    "  return df\r\n",
    "\r\n",
    "\r\n",
    "columnlist=[]\r\n",
    "for i in range(256):\r\n",
    "    columnlist.append('usernamehash_col'+str(i))\r\n",
    "columnlist+=['Timestamp','#Followers','#Friends','Retweets',\"#Favourites\",'Mentions_count',\"Hashtag_counts\",'URL_counts','confidence_mean','confidence_median','confidence_max']\r\n",
    "for i in range(200):\r\n",
    "    columnlist.append(\"Embeddings_\"+str(i))\r\n",
    "for i in range(2):\r\n",
    "    columnlist.append('Sentiments'+str(i))\r\n",
    "for i in range(64):\r\n",
    "    columnlist.append('Mentionshash_col'+str(i))\r\n",
    "for i in range(64):\r\n",
    "    columnlist.append('Hashtagshash_col'+str(i))\r\n",
    "for i in range(256):\r\n",
    "    columnlist.append('URLshash_col'+str(i))        \r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/keith_goh/newenv/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use Ray to parallel process the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "import numpy as np\r\n",
    "import psutil\r\n",
    "import ray\r\n",
    "import scipy.signal\r\n",
    "\r\n",
    "num_cpus = psutil.cpu_count(logical=False)\r\n",
    "\r\n",
    "ray.init(num_cpus=num_cpus)\r\n",
    "\r\n",
    "@ray.remote\r\n",
    "def cleanall(name):\r\n",
    "    print(name)\r\n",
    "    print('processed'+name[7:])\r\n",
    "    df=pd.read_feather(name)\r\n",
    "    df=df.drop('level_0',axis=1)\r\n",
    "    df=df.drop('index',axis=1)\r\n",
    "    print('1')\r\n",
    "    df=hash_encode_username(df)\r\n",
    "    print('2')\r\n",
    "    df=clean_timestamp(df)\r\n",
    "    print('3')\r\n",
    "    df=create_entitylist(df)\r\n",
    "    print('4')\r\n",
    "    df= split_sentiment(df)\r\n",
    "    print('5')\r\n",
    "    df=clean_general(df,\"Mentions\")\r\n",
    "    print('6')\r\n",
    "    df=count_general(df,'Mentions')\r\n",
    "    print('7')\r\n",
    "    df=clean_general(df,\"Hashtags\")  \r\n",
    "    print('8')\r\n",
    "    df=count_general(df,'Hashtags')\r\n",
    "    print('9')\r\n",
    "    df=clean_url(df)\r\n",
    "    print('10')\r\n",
    "    df=count_url(df)\r\n",
    "    print('11')\r\n",
    "    df=split_timestamp_to_cols(df)\r\n",
    "    df=df.drop('Tweet_Id',axis=1)\r\n",
    "    df.columns=columnlist\r\n",
    "    df.to_feather('processed3'+name[7:])\r\n",
    "corpus = api.load('glove-twitter-200')\r\n",
    "stored_model=ray.put(corpus)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-29 03:58:43,711\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "files =  glob.glob('./split/*.ftr')\r\n",
    "random.shuffle(files)\r\n",
    "for file in files:\r\n",
    "    cleanall.remote(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}