{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import pandas as pd\r\n",
    "from sklearn.metrics import mean_squared_log_error\r\n",
    "import torch.nn as nn\r\n",
    "from torch.utils.data import TensorDataset, DataLoader\r\n",
    "import torch\r\n",
    "import pandas as pd\r\n",
    "from numpy import array\r\n",
    "from tqdm import tqdm\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "files=pd.read_json('test_files_801010.json')\r\n",
    "# change path to reflect where the data is \r\n",
    "#and which data you are testing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "len(files)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19967"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "is_cuda = torch.cuda.is_available()\r\n",
    "\r\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\r\n",
    "if is_cuda:\r\n",
    "    device = torch.device(\"cuda\")\r\n",
    "else:\r\n",
    "    device = torch.device(\"cpu\")\r\n",
    "\r\n",
    "\r\n",
    "def modelload(param,path_to_model):\r\n",
    "\r\n",
    "    class GenModel(nn.Module):\r\n",
    "        \"\"\"[LSTM Model Generator]\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        def __init__(self, hidden_dim,seq_length, n_layers,hidden_layers,\r\n",
    "                    bidirectional, dropout=0.5):\r\n",
    "            \"\"\"[summary]\r\n",
    "\r\n",
    "            Args:\r\n",
    "                hidden_dim ([List]): [list of integers for dimensions of hidden layers]\r\n",
    "                seq_length ([int]): [window size of 1 reading]\r\n",
    "                n_layers ([int]): [description]\r\n",
    "                hidden_layers ([int]): [description]\r\n",
    "                bidirectional ([boolean]): [boolean of whether the bidirectional ]\r\n",
    "                dropout (float, optional): [description]. Defaults to 0.5.\r\n",
    "            \"\"\"\r\n",
    "            super().__init__()\r\n",
    "            self.rnn = nn.LSTM(856, \r\n",
    "                            hidden_dim[0], \r\n",
    "                            num_layers=n_layers, #set to two: makes our LSTM 'deep'\r\n",
    "                            bidirectional=bidirectional, #bidirectional or not\r\n",
    "                            dropout=dropout,batch_first=True) #we add dropout for regularization\r\n",
    "            \r\n",
    "            if bidirectional:\r\n",
    "                self.D=2\r\n",
    "            else:\r\n",
    "                self.D=1\r\n",
    "            self.n_layers=n_layers\r\n",
    "            self.hidden_dim=hidden_dim[0]\r\n",
    "            self.nonlinearity = nn.ReLU() \r\n",
    "            self.hidden_layers = nn.ModuleList([])\r\n",
    "            self.seq_length=seq_length\r\n",
    "            self.dropout=nn.Dropout(dropout)\r\n",
    "            assert(len(hidden_dim)>0)\r\n",
    "            assert(len(hidden_dim)==1+hidden_layers)\r\n",
    "\r\n",
    "            i=0\r\n",
    "            if hidden_layers>0:\r\n",
    "                self.hidden_layers.append(nn.Linear(hidden_dim[i]*self.D*self.seq_length, hidden_dim[i+1]))\r\n",
    "                for i in range(hidden_layers-1):\r\n",
    "                    self.hidden_layers.append(nn.Linear(hidden_dim[i+1], hidden_dim[i+2]))\r\n",
    "                self.output_projection = nn.Linear(hidden_dim[i+1], 1)\r\n",
    "            else:\r\n",
    "                self.output_projection = nn.Linear(hidden_dim[i]*self.D*self.seq_length, 1)\r\n",
    "        \r\n",
    "            \r\n",
    "            \r\n",
    "        def forward(self, x,hidden):\r\n",
    "            \"\"\"[Forward for Neural network]\r\n",
    "\r\n",
    "            Args:\r\n",
    "                x ([Tensor]): [input tensor for raw values]\r\n",
    "                hidden ([Tensor]): [hidden state values for lstm model]\r\n",
    "\r\n",
    "            Returns:\r\n",
    "                [Tensor]: [output results from model]\r\n",
    "            \"\"\"\r\n",
    "            \r\n",
    "            batch_size= x.size(0)\r\n",
    "\r\n",
    "            val, hidden = self.rnn(x,hidden) #feed to rnn\r\n",
    "            \r\n",
    "            #unpack sequence\r\n",
    "            val = val.contiguous().view( batch_size,-1)\r\n",
    "            for hidden_layer in self.hidden_layers:\r\n",
    "                val = hidden_layer(val)\r\n",
    "                val = self.dropout(val)\r\n",
    "                val = self.nonlinearity(val) \r\n",
    "            out = self.output_projection(val)\r\n",
    "\r\n",
    "            return out,hidden\r\n",
    "        \r\n",
    "        \r\n",
    "        def init_hidden(self, batch_size):\r\n",
    "            \"\"\"[summary]\r\n",
    "\r\n",
    "            Args:\r\n",
    "                batch_size ([int]): [size of batch that you are inputting into the model]\r\n",
    "\r\n",
    "            Returns:\r\n",
    "                [Tensor]: [Returns a tensor with the dimensions equals to the dimensions of the model's\r\n",
    "                hidden state with values 0]\r\n",
    "            \"\"\"\r\n",
    "                weight = next(self.parameters()).data\r\n",
    "                hidden = (weight.new(self.n_layers*self.D, batch_size, self.hidden_dim).zero_().to(device),\r\n",
    "                            weight.new(self.n_layers*self.D, batch_size, self.hidden_dim).zero_().to(device))\r\n",
    "                \r\n",
    "                return hidden\r\n",
    "\r\n",
    "\r\n",
    "    newmodel = Genmodel(param[0],param[1],param[2],param[3],param[4]).double()\r\n",
    "    newmodel.to(device)\r\n",
    "    newmodel.load_state_dict(torch.load(path_to_model))\r\n",
    "    return newmodel\r\n",
    "\r\n",
    "\r\n",
    " \r\n",
    "   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def split_sequences(sequences, n_steps):\r\n",
    "    X, y = list(), list()\r\n",
    "    for i in range(len(sequences)):\r\n",
    "        # find the end of this pattern\r\n",
    "        end_ix = i + n_steps\r\n",
    "        # check if we are beyond the dataset\r\n",
    "        if end_ix > len(sequences):\r\n",
    "            break\r\n",
    "        # gather input and output parts of the pattern\r\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\r\n",
    "        X.append(seq_x)\r\n",
    "        y.append(seq_y)\r\n",
    "    return array(X), array(y)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "newmodel= modelload(([256], 30,2, 0, True,0.5),'./state_dict_11.pt')\r\n",
    "newmodel.eval()\r\n",
    "stepsize=40\r\n",
    "\r\n",
    "n_timesteps=30\r\n",
    "batch_size = 100-n_timesteps+1\r\n",
    "epoch_val=files[0]\r\n",
    "epoch_size=len(files[0])\r\n",
    "listmean=[]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\n",
    "for number in tqdm(range(int(epoch_size/stepsize))):\n",
    "    val_x= np.empty((0,n_timesteps,856), int)\n",
    "    val_y= np.empty((0,), int)\n",
    "    startno=number*stepsize\n",
    "    for i in (epoch_val[startno:startno+stepsize]):\n",
    "        joineddf=pd.read_feather('processed3-edited/'+i)\n",
    "        joineddf=joineddf.fillna(0)\n",
    "        tnp=joineddf[[c for c in joineddf if c not in ['Retweets']] \n",
    "               + ['Retweets']].to_numpy()\n",
    "        valnpx,valnpy=split_sequences(tnp, n_timesteps)\n",
    "\n",
    "        val_x = np.append(val_x, valnpx, axis=0)\n",
    "        val_y = np.append(val_y, valnpy, axis=0)\n",
    "    val_x=torch.Tensor(val_x).double().to(device)\n",
    "    h = newmodel.init_hidden(val_x.size()[0])\n",
    "    hcon = tuple([e.data for e in h])\n",
    "    predictions = newmodel(val_x,hcon)\n",
    "    listmean.append(mean_squared_log_error(val_y, predictions[0].cpu().detach().numpy().clip(min=0)))\n",
    "    predictions=[]\n",
    "    pd.DataFrame(listmean).to_csv('./test/mean'+str(number)+'.csv')    \n",
    "           "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 499/499 [1:45:02<00:00, 12.63s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "listmean"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.8019755086526017,\n",
       " 1.7719247183923597,\n",
       " 1.6814075940387028,\n",
       " 1.8234251791496432,\n",
       " 1.5685152627706276,\n",
       " 1.9288396539766304,\n",
       " 1.8057732393242756,\n",
       " 1.729193007186856,\n",
       " 1.6460566107142824,\n",
       " 1.8160024687370961,\n",
       " 1.9705065634158832,\n",
       " 1.6617258032230984,\n",
       " 1.8312713397611506,\n",
       " 1.786196308293331,\n",
       " 1.8535799482182356,\n",
       " 1.7394661567161962,\n",
       " 1.8360308858710441,\n",
       " 1.76464875729408,\n",
       " 1.8626188604641087,\n",
       " 1.6913838151301104,\n",
       " 1.7362902204658082,\n",
       " 1.6934517766171449,\n",
       " 1.8838662181283428,\n",
       " 1.8019107302221586,\n",
       " 1.7259096368121263,\n",
       " 1.7371824211427311,\n",
       " 2.0335833910230834,\n",
       " 1.7354343523513736,\n",
       " 1.8524574320631826,\n",
       " 1.8246450919427315,\n",
       " 1.906096252129842,\n",
       " 1.7831248220760123,\n",
       " 1.7102467476442729,\n",
       " 1.7575557981231902,\n",
       " 1.6862480578747883,\n",
       " 1.8186435221575503,\n",
       " 1.7008278128575451,\n",
       " 1.8206525068103379,\n",
       " 1.8873227308975062,\n",
       " 1.648774843033898,\n",
       " 1.9226458818591043,\n",
       " 1.6299314899940738,\n",
       " 1.861519838170586,\n",
       " 1.7115538619066657,\n",
       " 1.8744629695408472,\n",
       " 1.5554035692061698,\n",
       " 1.7449606620144855,\n",
       " 1.7075200832072222,\n",
       " 1.8325533276785893,\n",
       " 1.8753620483470295,\n",
       " 1.8254651129900012,\n",
       " 1.6860805573356212,\n",
       " 1.9789032302388863,\n",
       " 1.7570033303785348,\n",
       " 1.7850804382664849,\n",
       " 1.8010834267211833,\n",
       " 1.666195851550578,\n",
       " 1.7333946419778494,\n",
       " 1.783112360373721,\n",
       " 1.7971539206642826,\n",
       " 1.8846851113286722,\n",
       " 1.8506677661565,\n",
       " 1.7603651938824412,\n",
       " 1.6857115790398618,\n",
       " 1.6998898771227657,\n",
       " 1.8329952653658297,\n",
       " 1.7778699871186934,\n",
       " 1.8608482564960875,\n",
       " 1.8438322191118632,\n",
       " 1.8889405512856383,\n",
       " 1.6322497618720515,\n",
       " 1.7720320602674895,\n",
       " 1.7725473651403565,\n",
       " 1.8951113520265441,\n",
       " 1.840545687070298,\n",
       " 1.8501548829397574,\n",
       " 1.8776895451431532,\n",
       " 1.7827765271124265,\n",
       " 1.7652403921889632,\n",
       " 2.0706594305653176,\n",
       " 1.7695779025193261,\n",
       " 1.835604590306963,\n",
       " 1.7272556062643374,\n",
       " 1.675317852428805,\n",
       " 1.772687655845647,\n",
       " 1.8702259895602344,\n",
       " 1.7546553968185177,\n",
       " 1.9129637332757572,\n",
       " 1.7954460767255498,\n",
       " 1.8921548164510724,\n",
       " 1.6996187737185842,\n",
       " 1.8128962921173362,\n",
       " 1.8298200521051589,\n",
       " 1.8261170710518235,\n",
       " 1.7843923395678412,\n",
       " 1.7197019195483785,\n",
       " 1.7555798916882772,\n",
       " 1.7629216564418708,\n",
       " 1.7721525329484034,\n",
       " 1.7499812458980357,\n",
       " 1.7344464757594442,\n",
       " 1.7374478383579635,\n",
       " 1.8087841461941025,\n",
       " 1.9108265414660803,\n",
       " 1.7515008583352056,\n",
       " 1.7640479540830838,\n",
       " 1.8536652258564899,\n",
       " 1.7809480475076584,\n",
       " 1.7985213665378965,\n",
       " 1.7890740579109963,\n",
       " 1.5712009679895538,\n",
       " 1.908918787185337,\n",
       " 1.8050495641148898,\n",
       " 1.8095611519448185,\n",
       " 1.6690994734321467,\n",
       " 1.8953249081726529,\n",
       " 1.7264701711993247,\n",
       " 1.7025103959134893,\n",
       " 1.9728309334387526,\n",
       " 1.7546722386970832,\n",
       " 1.896087887098701,\n",
       " 1.6590512302807043,\n",
       " 1.8027651053084404,\n",
       " 1.7567923941005197,\n",
       " 1.7042300537443535,\n",
       " 1.8554423862104283,\n",
       " 1.9559392510773994,\n",
       " 1.7587578829445103,\n",
       " 1.821237826446853,\n",
       " 1.6861887356855736,\n",
       " 1.9728207657056973,\n",
       " 1.7494291480090411,\n",
       " 2.0320011543410748,\n",
       " 1.8224975341592051,\n",
       " 1.8009180136480116,\n",
       " 1.843093145847343,\n",
       " 1.8808910385517474,\n",
       " 1.6935283996839894,\n",
       " 1.6268464662768978,\n",
       " 1.8184734222334622,\n",
       " 1.8807602714464289,\n",
       " 1.7787514466567278,\n",
       " 1.672148033016348,\n",
       " 1.8796848262864547,\n",
       " 1.8439049347624847,\n",
       " 1.8358691853483091,\n",
       " 1.9370652847413359,\n",
       " 1.6661971420265713,\n",
       " 2.0300909649755448,\n",
       " 2.022141148415944,\n",
       " 1.7354761634632019,\n",
       " 1.9161630384465833,\n",
       " 1.7916153438883708,\n",
       " 1.879133428213887,\n",
       " 1.6283633806327018,\n",
       " 2.0330513783347293,\n",
       " 1.7413323798915352,\n",
       " 1.8086925910032876,\n",
       " 1.7407750299125626,\n",
       " 1.658937267856461,\n",
       " 1.88299511918802,\n",
       " 1.6802088997354105,\n",
       " 1.8058681011773683,\n",
       " 1.947859566913777,\n",
       " 1.688675833268122,\n",
       " 1.9944993586246922,\n",
       " 1.7282436314927063,\n",
       " 1.6708907499972463,\n",
       " 1.756917372209807,\n",
       " 1.9166421328386525,\n",
       " 1.7436959553090168,\n",
       " 1.6675113699056383,\n",
       " 1.921162564194193,\n",
       " 1.7632847350137069,\n",
       " 1.769317752716937,\n",
       " 1.8180881886376585,\n",
       " 1.7586628191992761,\n",
       " 1.7952689843701282,\n",
       " 1.7169863940802395,\n",
       " 1.986088386189215,\n",
       " 1.7689069269689939,\n",
       " 1.761837746855138,\n",
       " 1.9104514108347956,\n",
       " 2.0442503634585947,\n",
       " 1.6577543145272986,\n",
       " 1.7218487403688292,\n",
       " 1.7178360422896493,\n",
       " 1.681749920873321,\n",
       " 1.7665707132465363,\n",
       " 1.667445737491464,\n",
       " 1.839976680762852,\n",
       " 1.8435028126101942,\n",
       " 1.8409149461710677,\n",
       " 1.828783644269732,\n",
       " 1.8223705570852347,\n",
       " 2.1709484983302674,\n",
       " 1.8262524580784623,\n",
       " 1.9947061867480167,\n",
       " 1.7899400761099835,\n",
       " 1.7056202368200246,\n",
       " 1.8011533923646799,\n",
       " 1.8300220420638182,\n",
       " 1.7403984874892788,\n",
       " 1.6091124449945313,\n",
       " 1.7185085598981424,\n",
       " 1.7241031711828891,\n",
       " 1.6049982768839148,\n",
       " 1.9570411343353102,\n",
       " 1.555024060432204,\n",
       " 1.8630883348228646,\n",
       " 1.9160365722377775,\n",
       " 1.5874067504001892,\n",
       " 1.769958141120467,\n",
       " 1.8285745767813577,\n",
       " 1.8410539008383027,\n",
       " 1.6750692889074819,\n",
       " 1.9301689640285482,\n",
       " 1.9614694637805536,\n",
       " 1.6089599752689312,\n",
       " 1.7734373385045747,\n",
       " 1.8395764078836907,\n",
       " 1.799085075094962,\n",
       " 1.808684832762487,\n",
       " 1.7230506610038667,\n",
       " 1.9181368252514543,\n",
       " 1.7854328487590232,\n",
       " 1.8951925095073325,\n",
       " 1.7859692421119855,\n",
       " 1.8903795960179868,\n",
       " 1.7826434393987445,\n",
       " 1.7122217425934503,\n",
       " 1.7725193579973504,\n",
       " 1.784029939030074,\n",
       " 1.8228704650784666,\n",
       " 1.8732708904145776,\n",
       " 1.9544561957054534,\n",
       " 1.7903255331021612,\n",
       " 1.8328978561394083,\n",
       " 1.8018979940409332,\n",
       " 1.7652778479028683,\n",
       " 1.665424381087665,\n",
       " 1.7249468424988432,\n",
       " 1.890549688172545,\n",
       " 1.8210379411724882,\n",
       " 1.7725609359098196,\n",
       " 1.7613442910786892,\n",
       " 1.8215354490410525,\n",
       " 1.8426897807001643,\n",
       " 1.6953230743347216,\n",
       " 1.7892872345655286,\n",
       " 1.9431073538982315,\n",
       " 1.829124317962714,\n",
       " 1.6439580831701643,\n",
       " 1.7497057021079296,\n",
       " 1.7849243863772828,\n",
       " 1.6390306293584402,\n",
       " 1.7756148544267034,\n",
       " 1.6087414390567856,\n",
       " 1.7908585759298392,\n",
       " 1.79526953451684,\n",
       " 1.7764383622138644,\n",
       " 1.7577486336617063,\n",
       " 1.762455069413851,\n",
       " 1.774328472931734,\n",
       " 1.8589034512382747,\n",
       " 1.870291634367038,\n",
       " 1.7785968345392364,\n",
       " 1.9250985957307913,\n",
       " 1.7686816232011202,\n",
       " 2.0034427668331234,\n",
       " 1.8501582019252,\n",
       " 1.8652648523896027,\n",
       " 1.6930146200673033,\n",
       " 1.853718098567809,\n",
       " 1.801202289397133,\n",
       " 1.8107398383685687,\n",
       " 1.9235307353513331,\n",
       " 1.912880938823802,\n",
       " 1.6708741809453174,\n",
       " 1.805586284111771,\n",
       " 1.8512788060625482,\n",
       " 1.8549071530741374,\n",
       " 1.8829442026409369,\n",
       " 1.8758201113023423,\n",
       " 1.785296050459147,\n",
       " 1.769570751624875,\n",
       " 1.7959401374773625,\n",
       " 1.8867563187880791,\n",
       " 1.670960005153951,\n",
       " 1.8628281624445369,\n",
       " 1.7424091301758355,\n",
       " 1.912878980793324,\n",
       " 1.8033669841517097,\n",
       " 1.7296584126564158,\n",
       " 1.7318660416638068,\n",
       " 1.7300209849782415,\n",
       " 1.7333488050054877,\n",
       " 1.7505683987915124,\n",
       " 1.7975005328142442,\n",
       " 1.874962540634093,\n",
       " 1.813022282622781,\n",
       " 1.8055155549646187,\n",
       " 1.8963137446343026,\n",
       " 1.7332082859178064,\n",
       " 1.6038728783767162,\n",
       " 1.8651552541423992,\n",
       " 1.8591467866243054,\n",
       " 2.0544396625171064,\n",
       " 1.7609312540751079,\n",
       " 1.9831288125020314,\n",
       " 1.7831315758889477,\n",
       " 1.847513607773381,\n",
       " 1.868141889599406,\n",
       " 1.748793392827221,\n",
       " 1.8597124618320018,\n",
       " 1.7542181724130703,\n",
       " 1.8435280746215552,\n",
       " 1.7757598976839422,\n",
       " 1.6966166658693005,\n",
       " 1.831266185428941,\n",
       " 1.7776910092482245,\n",
       " 2.0034092148290124,\n",
       " 1.9301892261552656,\n",
       " 1.7245789895706238,\n",
       " 1.931859583531284,\n",
       " 1.7649894926770506,\n",
       " 1.6525232155852938,\n",
       " 1.7271514027150774,\n",
       " 1.6961870716433187,\n",
       " 1.8491060655199298,\n",
       " 1.881969868008,\n",
       " 1.7613035500147503,\n",
       " 1.7868425166006263,\n",
       " 1.937294614282498,\n",
       " 1.677120310153361,\n",
       " 1.9268450128850143,\n",
       " 1.755177410813514,\n",
       " 1.7595043748061225,\n",
       " 1.7475010555880355,\n",
       " 1.8118719852208023,\n",
       " 1.7893797146822443,\n",
       " 1.881174850574297,\n",
       " 1.7996646803694876,\n",
       " 1.8188665462080613,\n",
       " 1.8124384630634902,\n",
       " 1.701570378504625,\n",
       " 1.943853540898507,\n",
       " 1.643598555648777,\n",
       " 1.753183135595967,\n",
       " 1.7350888415299153,\n",
       " 1.835814012381485,\n",
       " 1.6682854009954926,\n",
       " 1.909773190231519,\n",
       " 1.7788204710779023,\n",
       " 1.7762420320904206,\n",
       " 1.9151740054817765,\n",
       " 1.7411784256921679,\n",
       " 1.728205691364881,\n",
       " 1.7252208913181324,\n",
       " 1.7611417103257463,\n",
       " 1.8614671507550755,\n",
       " 1.8201257427789146,\n",
       " 1.7778829527337332,\n",
       " 1.880068559985918,\n",
       " 1.7275122710252981,\n",
       " 1.7164338833643553,\n",
       " 1.8625376301381449,\n",
       " 1.7448220045232126,\n",
       " 1.7839611374892945,\n",
       " 1.9349697295493353,\n",
       " 1.9102950228649367,\n",
       " 1.7214013740004084,\n",
       " 1.976963805841141,\n",
       " 1.9671500824201686,\n",
       " 1.6904965752307946,\n",
       " 1.8614008572146514,\n",
       " 1.6136133098784122,\n",
       " 1.871761041156014,\n",
       " 1.825781641254818,\n",
       " 1.7036984813125804,\n",
       " 1.836626068040828,\n",
       " 1.769351221610252,\n",
       " 1.7982968407252,\n",
       " 1.60543308147831,\n",
       " 1.7273435587146346,\n",
       " 1.798202372188874,\n",
       " 1.8285726233398505,\n",
       " 1.852761700179987,\n",
       " 1.770403980328142,\n",
       " 1.8389233594256298,\n",
       " 1.7933837762487272,\n",
       " 1.802485667767342,\n",
       " 1.7693135188527598,\n",
       " 1.7813400162689588,\n",
       " 1.8073873464217658,\n",
       " 1.719774909803372,\n",
       " 1.8302961095432517,\n",
       " 1.7669042163008284,\n",
       " 1.8626315254521455,\n",
       " 1.6207145867988402,\n",
       " 1.852851852140605,\n",
       " 1.823569483344868,\n",
       " 1.7714187004773931,\n",
       " 2.1004031000656735,\n",
       " 1.829993858469506,\n",
       " 1.8001333764916705,\n",
       " 1.7098672345914008,\n",
       " 1.8199995608540498,\n",
       " 1.8137407009270115,\n",
       " 1.8741962485191188,\n",
       " 1.8109244116606011,\n",
       " 1.790158311471556,\n",
       " 1.8672913260811064,\n",
       " 1.7281351336629145,\n",
       " 1.6374877060481505,\n",
       " 1.8286167029609413,\n",
       " 1.8648392879286364,\n",
       " 1.6799190797341228,\n",
       " 1.980102988339902,\n",
       " 1.774225287194834,\n",
       " 1.7177929423276561,\n",
       " 1.9057150678188477,\n",
       " 1.898097174056575,\n",
       " 1.8193421385334643,\n",
       " 1.735109301430929,\n",
       " 1.754623469768283,\n",
       " 1.8206831290332963,\n",
       " 1.7634478202272814,\n",
       " 2.0100783580053316,\n",
       " 1.6243194915926131,\n",
       " 1.7322361168498228,\n",
       " 1.8115932452377648,\n",
       " 1.966711946945465,\n",
       " 1.769115843260716,\n",
       " 1.7132371961732815,\n",
       " 1.9179169851438576,\n",
       " 1.8219675930084036,\n",
       " 1.6996161759044157,\n",
       " 1.706105541861295,\n",
       " 1.8776551751958797,\n",
       " 1.9791994259433674,\n",
       " 1.676144082021522,\n",
       " 1.906888739582794,\n",
       " 1.8184347498279096,\n",
       " 1.8367280380781446,\n",
       " 1.7646012663568849,\n",
       " 1.7994878664591554,\n",
       " 1.8434005606744561,\n",
       " 1.871220856994413,\n",
       " 1.6773595653791795,\n",
       " 1.6534134823112219,\n",
       " 1.7532631176291495,\n",
       " 1.657946316468928,\n",
       " 1.7863022786283453,\n",
       " 1.8066727727814853,\n",
       " 1.9411865968571174,\n",
       " 1.7411488649410227,\n",
       " 1.8429109505343793,\n",
       " 1.8572009697586644,\n",
       " 1.6849938235334192,\n",
       " 1.7786519458401409,\n",
       " 1.5802197384195067,\n",
       " 1.7850919222087365,\n",
       " 1.782865424344659,\n",
       " 1.839614510820784,\n",
       " 1.7749556656303893,\n",
       " 1.759147766187722,\n",
       " 1.6853307323667355,\n",
       " 1.8647110420564632,\n",
       " 1.8187127389915794,\n",
       " 1.7219297914280844,\n",
       " 1.8530934117347095,\n",
       " 1.896487933485942,\n",
       " 1.7823795776486522,\n",
       " 1.7670561225045653,\n",
       " 1.6829889945105379,\n",
       " 1.8394368868366822,\n",
       " 1.811943475780323,\n",
       " 1.8546280764110519,\n",
       " 1.8563862895392595,\n",
       " 1.7792056617022902,\n",
       " 1.8176839556597606,\n",
       " 1.7176698640612469,\n",
       " 1.8277757494795512,\n",
       " 1.8406987977342812,\n",
       " 1.80222186624468,\n",
       " 1.709313250387863,\n",
       " 1.9259940474543442,\n",
       " 1.823993966513073,\n",
       " 1.7545797017810483,\n",
       " 1.6667350937412186,\n",
       " 1.868385502795872,\n",
       " 1.7580315590483653,\n",
       " 1.8058430796482254,\n",
       " 1.8565129273442689,\n",
       " 1.7918695532095597,\n",
       " 1.8221966030348815,\n",
       " 1.7546151651886728,\n",
       " 1.8611951026156328]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "np.mean(listmean)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.798414006329342"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}