{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pip install pyarrow=1.0.0\r\n",
    "#in case you cannot read feather"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import glob\r\n",
    "import torch\r\n",
    "from tqdm import tqdm\r\n",
    "from numpy import array\r\n",
    "import numpy as np\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\r\n",
    "is_cuda = torch.cuda.is_available()\r\n",
    "\r\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\r\n",
    "if is_cuda:\r\n",
    "    device = torch.device(\"cuda\")\r\n",
    "else:\r\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn as nn\r\n",
    "\r\n",
    "class GenModel(nn.Module):\r\n",
    "    \"\"\"[LSTM Model Generator]\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, hidden_dim,seq_length, n_layers,hidden_layers,\r\n",
    "                 bidirectional, dropout=0.5):\r\n",
    "        \"\"\"[summary]\r\n",
    "\r\n",
    "        Args:\r\n",
    "            hidden_dim ([List]): [list of integers for dimensions of hidden layers]\r\n",
    "            seq_length ([int]): [window size of 1 reading]\r\n",
    "            n_layers ([int]): [description]\r\n",
    "            hidden_layers ([int]): [description]\r\n",
    "            bidirectional ([boolean]): [boolean of whether the bidirectional ]\r\n",
    "            dropout (float, optional): [description]. Defaults to 0.5.\r\n",
    "        \"\"\"\r\n",
    "        super().__init__()\r\n",
    "        self.rnn = nn.LSTM(856, \r\n",
    "                           hidden_dim[0], \r\n",
    "                           num_layers=n_layers, #set to two: makes our LSTM 'deep'\r\n",
    "                           bidirectional=bidirectional, #bidirectional or not\r\n",
    "                           dropout=dropout,batch_first=True) #we add dropout for regularization\r\n",
    "        \r\n",
    "        if bidirectional:\r\n",
    "            self.D=2\r\n",
    "        else:\r\n",
    "            self.D=1\r\n",
    "        self.n_layers=n_layers\r\n",
    "        self.hidden_dim=hidden_dim[0]\r\n",
    "        self.nonlinearity = nn.ReLU() \r\n",
    "        self.hidden_layers = nn.ModuleList([])\r\n",
    "        self.seq_length=seq_length\r\n",
    "        self.dropout=nn.Dropout(dropout)\r\n",
    "        assert(len(hidden_dim)>0)\r\n",
    "        assert(len(hidden_dim)==1+hidden_layers)\r\n",
    "\r\n",
    "        i=0\r\n",
    "        if hidden_layers>0:\r\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dim[i]*self.D*self.seq_length, hidden_dim[i+1]))\r\n",
    "            for i in range(hidden_layers-1):\r\n",
    "                self.hidden_layers.append(nn.Linear(hidden_dim[i+1], hidden_dim[i+2]))\r\n",
    "            self.output_projection = nn.Linear(hidden_dim[i+1], 1)\r\n",
    "        else:\r\n",
    "            self.output_projection = nn.Linear(hidden_dim[i]*self.D*self.seq_length, 1)\r\n",
    "    \r\n",
    "        \r\n",
    "        \r\n",
    "    def forward(self, x,hidden):\r\n",
    "        \"\"\"[Forward for Neural network]\r\n",
    "\r\n",
    "        Args:\r\n",
    "            x ([Tensor]): [input tensor for raw values]\r\n",
    "            hidden ([Tensor]): [hidden state values for lstm model]\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            [Tensor]: [output results from model]\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        batch_size= x.size(0)\r\n",
    "\r\n",
    "        val, hidden = self.rnn(x,hidden) #feed to rnn\r\n",
    "        \r\n",
    "        #unpack sequence\r\n",
    "        val = val.contiguous().view( batch_size,-1)\r\n",
    "        for hidden_layer in self.hidden_layers:\r\n",
    "              val = hidden_layer(val)\r\n",
    "              val = self.dropout(val)\r\n",
    "              val = self.nonlinearity(val) \r\n",
    "        out = self.output_projection(val)\r\n",
    "\r\n",
    "        return out,hidden\r\n",
    "    \r\n",
    "    \r\n",
    "    def init_hidden(self, batch_size):\r\n",
    "        \"\"\"[summary]\r\n",
    "\r\n",
    "        Args:\r\n",
    "            batch_size ([int]): [size of batch that you are inputting into the model]\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            [Tensor]: [Returns a tensor with the dimensions equals to the dimensions of the model's\r\n",
    "            hidden state with values 0]\r\n",
    "        \"\"\"\r\n",
    "            weight = next(self.parameters()).data\r\n",
    "            hidden = (weight.new(self.n_layers*self.D, batch_size, self.hidden_dim).zero_().to(device),\r\n",
    "                          weight.new(self.n_layers*self.D, batch_size, self.hidden_dim).zero_().to(device))\r\n",
    "            \r\n",
    "            return hidden"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "newmodel = GenModel([512], 30,2, 0, True,0.5).double()\r\n",
    "newmodel.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def count_parameters(model):\r\n",
    "    \"\"\"[calculate the number of parameters the model has to train]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        model ([nn]): [input neural network]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [int]: [count of parameters that the model needs to train]\r\n",
    "    \"\"\"\r\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
    "\r\n",
    "print(f'The model has {count_parameters(newmodel):,} trainable parameters')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "files=pd.read_json('train_files_801010.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_sequences(sequences, n_steps):\r\n",
    "    \"\"\"[inputs a numpy array and outputs a windowed sequence to put into lstm model]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        sequences ([np.array]): [numpy array of data to be sequenced into windows]\r\n",
    "        n_steps ([int]): [window size for the model]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [np.array]: [returns a numpy array with data sequenced into windows of size n_steps]\r\n",
    "    \"\"\"\r\n",
    "    X, y = list(), list()\r\n",
    "    for i in range(len(sequences)):\r\n",
    "        # find the end of this pattern\r\n",
    "        end_ix = i + n_steps\r\n",
    "        # check if we are beyond the dataset\r\n",
    "        if end_ix > len(sequences):\r\n",
    "            break\r\n",
    "        # gather input and output parts of the pattern\r\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\r\n",
    "        X.append(seq_x)\r\n",
    "        y.append(seq_y)\r\n",
    "    return array(X), array(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random.seed(42) # seed the process for everyone to have the same files\r\n",
    "train_set=random.choices(list(files[0]),k=int(len(files)*0.8))\r\n",
    "val_set=list(set(files[0])-set(train_set))\r\n",
    "numberepochs=10 #number of epochs that we are using\r\n",
    "\r\n",
    "stepsize=75 # number of files processed each step\r\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\r\n",
    "optimizer = torch.optim.Adam(newmodel.parameters(), lr=0.001)\r\n",
    "n_timesteps=30 #size of window\r\n",
    "batch_size = 100-n_timesteps+1 #each batch size, we default it to 1 file per batch\r\n",
    "counter = 0 #counter to tell us when to print\r\n",
    "print_every = 50 #will print when counter reaches a multiple of this number\r\n",
    "clip = 5 #clips the norm of the gradient by this size if it exceeds, to preven exploding gradient problem\r\n",
    "valid_loss_min = np.Inf #loss\r\n",
    "\r\n",
    "trainloss=[]\r\n",
    "valloss=[]\r\n",
    "step=[]\r\n",
    "num=[]\r\n",
    "valbackup=[]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "for j in range(numberepochs):\r\n",
    "    epoch_train=random.choices(train_set,k=750*8)#80-20 split\r\n",
    "    epoch_val=random.choices(val_set,k=750*2)\r\n",
    "    print(j)\r\n",
    "    \r\n",
    "    for number in tqdm(range(int(750*8/stepsize))):\r\n",
    "        trainingnp_x= np.empty((0,30,856), int)\r\n",
    "        trainingnp_y= np.empty((0,), int)\r\n",
    "        startno=number*50\r\n",
    "        for i in (epoch_train[startno:startno+stepsize]):\r\n",
    "            joineddf=pd.read_feather('processed3-edited/'+i)\r\n",
    "            joineddf=joineddf.fillna(0)\r\n",
    "            tnp=joineddf[[c for c in joineddf if c not in ['Retweets']] \r\n",
    "                   + ['Retweets']].to_numpy()\r\n",
    "            trainingnpx,trainingnpy=split_sequences(tnp, n_timesteps)\r\n",
    "\r\n",
    "            trainingnp_x = np.append(trainingnp_x, trainingnpx, axis=0)\r\n",
    "            trainingnp_y = np.append(trainingnp_y, trainingnpy, axis=0)\r\n",
    "\r\n",
    "        valnp_x= np.empty((0,30,856), int)\r\n",
    "        valnp_y= np.empty((0,), int)\r\n",
    "        for i in (epoch_val[startno:startno+stepsize]):\r\n",
    "            joineddf=pd.read_feather('processed3-edited/'+i)\r\n",
    "            joineddf=joineddf.fillna(0)\r\n",
    "            vnp=joineddf[[c for c in joineddf if c not in ['Retweets']] #move retweets column to the end\r\n",
    "                   + ['Retweets']].to_numpy()\r\n",
    "            valnpx,valnpy=split_sequences(tnp, n_timesteps)\r\n",
    "\r\n",
    "            valnp_x = np.append(valnp_x, valnpx, axis=0)\r\n",
    "            valnp_y = np.append(valnp_y, valnpy, axis=0)\r\n",
    "        train_data = TensorDataset(torch.from_numpy(trainingnp_x), torch.from_numpy(trainingnp_y))\r\n",
    "        val_data = TensorDataset(torch.from_numpy(valnp_x), torch.from_numpy(valnp_y))\r\n",
    "        \r\n",
    "        train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\r\n",
    "        val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size)\r\n",
    "        newmodel.train()\r\n",
    "        h = newmodel.init_hidden(batch_size)\r\n",
    "        for inputs, labels in train_loader:\r\n",
    "            counter += 1\r\n",
    "            h = tuple([e.data for e in h])\r\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
    "            newmodel.zero_grad()\r\n",
    "            output, h = newmodel(inputs, h)\r\n",
    "            loss = criterion(output.squeeze(), labels)\r\n",
    "            loss.backward()\r\n",
    "            nn.utils.clip_grad_norm_(newmodel.parameters(), clip)## clips gradient if too large\r\n",
    "            optimizer.step()\r\n",
    "            if counter%print_every == 0:\r\n",
    "                val_h = newmodel.init_hidden(batch_size)\r\n",
    "                val_losses = []\r\n",
    "                newmodel.eval()\r\n",
    "                for inp, lab in val_loader:\r\n",
    "                    val_h = tuple([each.data for each in val_h])\r\n",
    "                    inp, lab = inp.to(device), lab.to(device)\r\n",
    "                    out, val_h = newmodel(inp, val_h)\r\n",
    "                    val_loss = criterion(out.squeeze(), lab)\r\n",
    "                    val_losses.append(val_loss.item())\r\n",
    "                    \r\n",
    "                newmodel.train()\r\n",
    "                print(\"Epoch: {}/{}...\".format(j+1, numberepochs),\r\n",
    "                        \"Step: {}...\".format(counter),\r\n",
    "                        \"Loss: {:.6f}...\".format(loss.item()),\r\n",
    "                        \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\r\n",
    "                step.append(counter)\r\n",
    "                num.append(j)\r\n",
    "                valloss.append(val_losses)\r\n",
    "                trainloss.append(loss.item)\r\n",
    "                valbackup.append(np.mean(val_losses))\r\n",
    "                if np.mean(val_losses) <= valid_loss_min:\r\n",
    "                    torch.save(newmodel.state_dict(), './state_dict_12.pt')\r\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\r\n",
    "                    valid_loss_min = np.mean(val_losses)\r\n",
    "                pd.DataFrame(valloss).to_csv('valloss.csv')\r\n",
    "                pd.DataFrame(num).to_csv('num.csv')\r\n",
    "                pd.DataFrame(step).to_csv('step.csv')\r\n",
    "                pd.DataFrame(valloss).to_csv('step.csv')\r\n",
    "                pd.DataFrame(valbackup).to_csv('valbackup.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(newmodel.state_dict(), './state_dict_13.pt')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}